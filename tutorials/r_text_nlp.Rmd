---
title: 'NLP Processing in R'
author: "Wouter van Atteveldt & Kasper Welbers"
date: "2019-04"
output: 
  github_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r, echo=F, message=F}
knitr::opts_chunk$set(echo = TRUE, results = FALSE, message = FALSE, warning = FALSE, fig.keep='none')
library(printr)
```

For text analysis it is often useful to POS tag and lemmatize your text, especially with non-English data.
R does not really have built-in functions for that, but there are libraries that connect to external tools to help you do this.
This handout first reviews spacy, which has a great R library and support for a number of (western) languages.
Then it will provide brief instructions for using UDpipe, which has native support in R.

# Spacyr

## Installing Spacyr

Spacy is a python package with processing models for 6 different languages, which makes it attractive to use if you need e.g. French or German lemmatizing.

To install it, you need to install the `spacy` module in python and download the appropriate language model. This depends on your computer and can be tricky to get right. The sections below give some instructions on how to do this depending on your OS, see https://spacy.io/usage/ for more information. 

### Installing Spacy using anaconda (esp. for Windows):

1. Install [anaconda](https://www.anaconda.com/)
2. In Anaconda navigator, create a virtual environment, e.g. named 'spacy', based on python 3
3. Once that environment is setup up, click the 'Play' button to open a terminal, and run: (replacing `de_core_news_sm` with the model you need)
```
conda install -c conda-forge spacy
python -m spacy download de_core_news_sm
```
tions on how to do this depending on your OS, see https://spacy.io/usage/ for more information. 

### Installing Spacy using pip (esp. for Mac/Linux):

1. Open a terminal (ubuntu: control-alt-T, mac: finder -> terminal)
2. For mac, install xcode using `xcode-select --install`
3. In the terminal, type the following commands to create a virtual environment, install spacy, and download the language model of your choice:
```
python3 -m venv spacy-env
spacy-env/bin/pip install spacy
spacy-env/bin/python -m spacy download de_core_news_sm
```

### Initializing spacyr

The next step is to initialize spacyr by pointing it to your installed environment. 

For anaconda, use the following (substituting the name you picked for the virtual environment and the language model of your choice):

```{r, eval=F}
library(spacyr)
spacy_initialize("de_core_news_sm", condaenv = "spacy")
```

For pip, use the following:

```{r, eval=F}
library(spacyr)
spacy_initialize("de_core_news_sm", python_executable = path.expand("~/spacy-env/bin/python"))
```

```{r, echo=F}
library(spacyr)
spacy_initialize("de_core_news_sm", python_executable =  path.expand("~/env/spacy/bin/python"))
```

### Troubleshooting

As said, getting spacy and spacyr installed can be tricky. If you get an error message on initializing spacyr, it might help to install spacyr and reticulate directly from source. Before installing, clean your environment (with the broomstick icon) and restart R using the Session menu. 

```{r}
if (!require(devtools)) install.packages(devtools)
install_github("rstudio/reticulate")
install_github("quanteda/spacyr")
```

If that doesn't help, you can try installnig spacy the other way (following the instructions above), and/or googling your error message. 

## Using Spacy(r)

With spacy successfully installed and spacyr initialized, we can now use it to parse text:

```{r}
tokens = spacy_parse("Ich bin ein Berliner")
head(tokens)
```



### Extracting information from spacy output

Before we go on, let's get a more realistic example: a German wikinews article about Volkswagen. 

```{r}
url = "https://gist.githubusercontent.com/vanatteveldt/bf9527ac6510e9b3e5c6b198b917ddd1/raw/45e6f6bfa0abba219935543eb70cca9f675703c7/VW_erneut_unter_Verdacht.txt"
library(readtext)
d = readtext(url)
d$text
```

We can parse it as earlier:

```{r}
tokens = spacy_parse(d$text, nounphrase = T)
head(tokens)
```

Of course, since the resulting tokens are simply a data frame, we can use our normal functions to e.g. list all verbs:

```{r}
library(tidyverse)
tokens %>% filter(pos=="VERB") %>% group_by(lemma) %>% summarize(n=n()) %>% arrange(-n)
tokens %>% filter(lemma == "stehen")
```

And we can quite easily recreate sentences as well by summarizing with the str_c function:

```{r}
tokens %>% filter(sentence_id == 3) %>% arrange(token_id) %>% summarize(sentence=str_c(token, collapse=" "))
tokens %>% filter(sentence_id == 3) %>% arrange(token_id) %>% summarize(sentence=str_c(lemma, pos, sep = "/", collapse=" "))
```

We can extract all entities (which combines multi-word entities):

```{r}
entity_extract(tokens)
entity_extract(tokens) %>% filter(entity_type=="ORG") %>% group_by(entity) %>% summarize(n=n())
```

Or all noun phrases (requires `nounphrase=T` in the parsing):

```{r}
nounphrase_extract(tokens)
```

You can also 'consolidate' the entities or nounphrases, meaning that the tokens will actually be replaced by them:

```{r}
tokens2 = entity_consolidate(tokens) 
head(tokens2)
tokens2 = nounphrase_consolidate(tokens) 
head(tokens2)
```

### Reading spacy output into Quanteda

It can be very useful to read spacy output back in quanteda. 
For example, we might wish to do a word cloud, dictionary analysis, or topic model of only the nouns in a text.

`spacyr` and `quanteda` are both developed by the group of Kenneth Benoit's group, so they are quite easy to integrate.

The `as.tokens` function transforms the tokens dataframe into a quanteda `tokens` object, which can then be used to create a dfm.

For example, this creates a word cloud of all lemmata:

```{r}
library(quanteda)
tokens %>% as.tokens(use_lemma=T) %>% dfm %>% textplot_wordcloud(min_count = 1)
```

It is often useful to filter on POS tag before creating the dfm. Unfortunately, you can't use the normal filter operation as that drops the information from the tokens object on which quanteda depends. However, you can extract the tokens with the POS, and then use that to filter:

```{r}
dfm_nouns = tokens %>% as.tokens(include_pos = "pos") %>%
  tokens_select(pattern = c("*/NOUN")) %>% dfm
dfm_nouns %>% textplot_wordcloud(min_count = 1)
```

If you wish to drop the `/NOUN` from the features afterwards, you can access the column names of the dfm directly:

```{r}
library(magrittr)
colnames(dfm_nouns) %<>% str_remove("/noun")
dfm_nouns %>% textplot_wordcloud(min_count = 1)
```

(note: the fancy notation `[x %<>% ...]` is the same as `[x = x %>% ...]`, but you need to library magrittr explicitly)

This also works with the entity consolidation:

```{r}
library(quanteda)
tokens %>% entity_consolidate %>% as.tokens(use_lemma=T, include_pos = "pos") %>% 
    tokens_select(pattern = c("*/NOUN", "*/ENTITY")) %>% dfm %>%
 textplot_wordcloud(min_count = 1)
```

## Closing the door

Spacyr keeps a python process running, which can consume quite a lot of memory. When you are done with spacy (but want to continue with R), you can finalize spacy:

```{r}
spacy_finalize()
```

After this, you will have to initialize spacy again before you can parse new text. 



# UDPipe

`udpipe` is an R package that can do many preprocessing steps for a variety of languages including English, French, German and Dutch. If you call it for a language you have not previously used, it will automatically download the language model.

For this example, we will use a very short text, as it can take (very) long to process large amounts of text. 

```{r}
small_text = c("Pelosi says Trump is welcome to testify in impeachment inquiry, if he chooses", "House speaker pushes back against presidentâ€™s accusations that process is stacked against him as Schumer echoes her suggestion")
```

Now, let's lemmatize and tag this corpus:

```{r}
library(udpipe)
tokens = udpipe(small_text, "english", parser="none")
head(tokens[,c('doc_id','sentence_id','token_id','token','lemma','upos')])
```

The output `tokens` contains the tokens in a data.frame format, where each row is a token and the columns provide information about the token (e.g., lemma, part-of-speech tag). 
This is also often referred to as a tokenlist representation.
As you can see, 'is' is lemmatized to 'be', and Pelosi and Trump are both recognized as proper nouns (names).

Now we can create a document-term matrix from these tokens. 
More specifically, we want to create a document-term matrix in the quanteda format (i.e. a document-feature matrix).
This is not a terribly complicated conversion, but it's also not trivial, especially if we want to also include document variables in the DTM.
For convenience, we therefore use the `corpustools` package.
This is a package that we developed ourselves for the main purpose of working with tokenlist data.
It therefore supports creating a tokenlist (using the `udpipe` package), and converting a tokenlist to a document-term matrix (with support for the `quanteda` dfm).

Like `quanteda`, corpustool can create a corpus from a data.frame, which makes it easy to include document variables.
Here we first convert a quanteda corpus to a data.frame, and then create a `corpustools` corpus using a udpipe model.
We'll only parse the first 5 documents for this example.

```{r}
library(corpustools)
corp_dt = convert(data_corpus_inaugural, 'data.frame')
tc = create_tcorpus(corp_dt[1:5,], udpipe_model = 'english-ewt')
```

The `tc` object is a tcorpus (token corpus), which contains both the tokenlist and the meta data.
With the `get_dfm` function, we can create a quanteda dfm. 
The only thing we need to specify is that we want to use the lemma for the columns.

```{r}
d = get_dfm(tc, 'lemma')
```

We can now also create a DTM for only a selection of part-of-speech tags. 
This can be done by either first subsetting the tcorpus, or by providing a subset
expression in the `get_dfm` function.
Here we creata a DTM containing only nouns and proper names, and as a simple example create a wordcloud of the most common verbs in the corpus.

```{r}
d = get_dfm(tc, 'lemma', subset_tokens = POS %in% c('NOUN','PROPN'))
textplot_wordcloud(d, max_words = 50)
```

