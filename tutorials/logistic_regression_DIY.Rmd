---
title: "Logistic regression: do it yourself"
author: "Kasper Welbers & Wouter van Atteveldt"
date: "January 2020"
output: 
  html_document:
    toc: yes
  github_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---


```{r, echo=F}
knitr::opts_chunk$set(warning=FALSE, results = FALSE, message=FALSE, fig.path = "img/")
library(printr)
```

# This tutorial

This tutorial shows you how to implement logistic regression yourself, using only basic R functions. 
The purpose is purely educational, since R perfectly well supports logistic regression.  
Throughout the tutorial we'll use some formulas, but don't worry if these make you uncomfortable. 
We'll try to clarify each formula by also showing the R code and using visualizations.

# From linear to logistic regression

Regression analysis is a technique used to predict the value of a *dependent* variable from one or multiple *independent* variables. 
Linear regression is used when the dependent variable is continuous, and a linear relation exists between the dependent and (the linear combination of) independent variables. 
A regression model with a single independent variable is then expressed as:

$$y_i = \beta_0 + \beta_1{x}_i + \epsilon_i$$
Here $y$ is the dependent variable and $x$ is the independent variable. 
The beta parameters represent the intercept $\beta_0$ and the regression coefficient $\beta_1$. 
Given the independent variable and the parameters, we *predict* $y$ as $\beta_0 + \beta_1{x}_i$.
The error of the prediction (i.e. the real $y$ minus the predicted $y$) is represented by the residual term $\epsilon$.
This residuals ($\epsilon_1$, $\epsilon_2$, ..., $\epsilon_n$) are assumed to be independent and normally distributed with mean 0. 

## Creating a toy dataset using the regression formula

A good way to develop an intuition for this formula is by using it to create a toy dataset. For this examples we set a random seed. This ensures that the results of the random sample are the same for everyone.

```{r}
set.seed(1)
```

First, we'll create $x$ by taking a random sample of 10 *observations* from a normal distribution, for which we can use the `rnorm` function.

```{r}
x = rnorm(n=200, mean=4, sd=3)
```

Now we decide what the intercept (b0) and regression coefficient (b1) should be.

```{r}
b0 = 3
b1 = 0.3
```

For the residuals we draw a random sample of length n from a normal distribution. The mean has to be 0, and the value of the standard deviation will determine how well $x$ can predict $y$ in our data. If standard deviation is zero (and thus the residual is zero for every observation) the prediction would be perfect. 

```{r}
e = rnorm(n=200, mean=0, sd=0.7)
```

Now we can use the regression formula to create $y$.

```{r}
y = b0 + b1*x + e
```

## Analyzing our toy dataset

Now that we have our toy data ($x$ and $y$) we can perform regression analysis.
Here we fit the model, print the model summary, and plot a scatterplot with the regression line.

```{r}
m = lm(y ~ x)
summary(m)
plot(x,y, col='grey')
abline(m, col='red')
```

We see that the coefficients ($\beta0 = 3.05$; $\beta1 = 0.29$) are indeed very close to the values we used  ($b0 = 3.05$; $b1 = 0.29$) to create the data (they're only not identical because after we added e this new and slightly different model is a better fit). 

A nice thing about this visualization is that it intuitively shows the structural and random part of the regression equation. The structural part is represented by the line, which is the predicted value:

$$3.05 + 0.29x_i$$
$$3.05 + 0.29x_i$$
The random part is the residual, which is represented as the distance between the dot of an observation, and the line. In other words, the residual (that which remains) is the observed value y, minus the predicted value:

$$e_i = 3.05 + 0.29x_i$$



```{r}
pred = m$fitted.values
resid = m$residuals

#i = 15
#points(x[i], y[i], col='blue', pch=10)
#segment()
```



Now let's visualize this as a scatterplot.

```{r}
plot(x,y)
```





Now that we have data (the $y$ and $x$ variables), we can apply regression analysis  




The purpose of regression analysis is to 


```{r}
plot(x,y)
```




To 




Here, Y is 

For a single independent variable, the 


With a single independent v

For example, 






Logistic regression is applied when the dependent variable is binary, meaning that it can only have the values 0 and 1. 


```{r}
?binomial
library(tidyverse)
d = mutate(iris, setosa = as.numeric(Species == 'setosa'))
plot(d$Sepal.Length, d$setosa)
m = lm(setosa ~ Sepal.Length, data=d)
abline(m)
```