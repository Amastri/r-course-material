---
title: "Fitting LDA Models in R"
author: "Wouter van Atteveldt & Kasper Welbers"
date: "November 2018"
output: 
  github_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---


```{r, echo=F}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, fig.path = "img/")
library(printr)
```

Fitting LDA models in R is technically quite simple: just call the `LDA` function from the `topicmodels` package.
First, let's create a document term matrix from the inaugural speeches in quanteda,
at the paragraph level since we can expect these to be mostly about the same topic:

```{r}
library(quanteda)
texts = corpus_reshape(data_corpus_inaugural, to = "paragraphs")
dfm = dfm(texts, remove_punct=T, remove=stopwords("english"))
dfm = dfm_trim(dfm, min_docfreq = 5)
```

To run LDA from a dfm, first convert to the topicmodels format, and then run LDA. 
Note the useof `set.seed(.)` to make sure that the analysis is reproducible. 

```{r}
library(topicmodels)
dtm = convert(dfm, to = "topicmodels") 
set.seed(1)
m = LDA(dtm, method = "Gibbs", k = 10,  control = list(alpha = 0.1))
m
```

## Inspecting LDA results

We can use `terms` to look at the top terms per topic:

```{r}
terms(m, 5)
```

The `posterior` function gives the posterior distribution of words and documents to topics,
which can be used to plot a word cloud of terms proportional to their occurrence:

```{r}
topic = 6
words = posterior(m)$terms[topic, ]
topwords = head(sort(words, decreasing = T), n=50)
head(topwords)
```

Now we can plot these words:
```{r lda-wordcloud}
library(wordcloud)
wordcloud(names(topwords), topwords)
```

We can also look at the topics per document, to find the top documents per topic:


```{r}
topic.docs = posterior(m)$topics[, topic] 
topic.docs = sort(topic.docs, decreasing=T)
head(topic.docs)
```

And we can find this document in the original texts by looking up the document id in the document variables `docvars`:

```{r}
docs = docvars(dfm)
topdoc = names(topic.docs)[1]
docid = which(rownames(docs) == topdoc)
texts[docid]
```

Finally, we can see which president prefered which topics:

```{r lda-heatmap}
docs = docs[rownames(docs) %in% rownames(dtm), ]
tpp = aggregate(posterior(m)$topics, by=docs["President"], mean)
rownames(tpp) = tpp$President
heatmap(as.matrix(tpp[-1]))
```

As you can see, the topics form a sort of 'block' distribution, with more modern presidents and older presidents
using quite different topics. So, either the role of presidents changed, or language use changed, or (probably) both.

To get a better fit of such temporal dynamics, see the session on *structural topic models*, which allow you to condition topic proportions and/or contents on metadata covariates such as source or date. 

## Visualizing LDA with LDAvis

`LDAvis` is a nice interactive visualization of LDA results.
It needs the LDA and DTM information in a slightly different format than what's readily available, but you can use the code below to create that format from the
lda model `m` and the `dtm`:

```{r, eval=F}
dtm = dtm[slam::row_sums(dtm) > 0, ]
phi = as.matrix(posterior(m)$terms)
theta <- as.matrix(posterior(m)$topics)
vocab <- colnames(phi)
doc.length = slam::row_sums(dtm)
term.freq = slam::col_sums(dtm)[match(vocab, colnames(dtm))]
library(LDAvis)
json = createJSON(phi = phi, theta = theta, vocab = vocab,
     doc.length = doc.length, term.frequency = term.freq)
serVis(json)
```
