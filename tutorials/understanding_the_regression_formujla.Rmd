---
title: "Logistic regression: do it yourself"
author: "Kasper Welbers & Wouter van Atteveldt"
date: "January 2020"
output: 
  html_document:
    toc: yes
  github_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---


```{r, echo=F}
knitr::opts_chunk$set(warning=FALSE, results = FALSE, message=FALSE, fig.path = "img/")
library(printr)
```

# This tutorial

This tutorial shows you how to implement logistic regression yourself, using only basic R functions. 
The purpose is purely educational, since R perfectly well supports logistic regression.  
Throughout the tutorial we'll use some formulas, but don't worry if these make you uncomfortable. 
We'll try to clarify each formula by also showing the R code and using visualizations.

# From linear to logistic regression

Regression analysis is a technique used to predict the value of a *dependent* variable from one or multiple *independent* variables. 
Linear regression is used when the dependent variable is continuous, and a linear relation exists between the dependent and (the linear combination of) independent variables. 
A regression model with a single independent variable is then expressed as:

$$y_i = \beta_0 + \beta_1{x}_i + \epsilon_i$$
Here $y$ is the dependent variable and $x$ is the independent variable. 
The beta parameters represent the intercept $\beta_0$ and the regression coefficient $\beta_1$. 
Given the independent variable and the parameters, we *predict* $y$ as $\beta_0 + \beta_1{x}_i$.
The error of the prediction (i.e. the real $y$ minus the predicted $y$) is represented by the residual term $\epsilon$.
This residuals ($\epsilon_1$, $\epsilon_2$, ..., $\epsilon_n$) are assumed to be independent and normally distributed with mean 0. 

## Creating a toy dataset using the regression formula

A good way to develop an intuition for this formula is by using it to create a toy dataset. For this examples we set a random seed. This ensures that the results of the random sample are the same for everyone.

```{r}
set.seed(1)
```

First, we'll create $x$ by taking a random sample of 10 *observations* from a normal distribution, for which we can use the `rnorm` function.

```{r}
x = rnorm(n=200, mean=4, sd=3)
```

Now we decide what the intercept (b0) and regression coefficient (b1) should be.

```{r}
b0 = 3
b1 = 0.3
```

For the residuals we draw a random sample of length n from a normal distribution. The mean has to be 0, and the value of the standard deviation will determine how well $x$ can predict $y$ in our data. If standard deviation is zero (and thus the residual is zero for every observation) the prediction would be perfect. 

```{r}
e = rnorm(n=200, mean=0, sd=0.7)
```

Now we can use the regression formula to create $y$.

```{r}
y = b0 + b1*x + e
```

## Analyzing our toy dataset

Now that we have our toy data ($x$ and $y$) we can perform regression analysis.
Here we fit the model, print the model summary, and plot a scatterplot with the regression line.

```{r}
m = lm(y ~ x)
summary(m)
plot(x,y, col='grey')
abline(m, col='green')
```

We see that the coefficients ($\beta0 = 3.05$; $\beta1 = 0.29$) are indeed very close to the values we used  ($b0 = 3.05$; $b1 = 0.29$) to create the data (they're only not identical because after we added e this new and slightly different model is a better fit). 

A nice thing about this visualization is that it intuitively distinguishes the structural and random part of the regression equation. The structural part is represented by the line, which is the predicted value:

$$\beta_0 + \beta_1{x}_i$$
$$3.05 + 0.29x_i$$
The random part is the residual, which is represented by the distance between the line and the dot. You can think of the residual (i.e. that which remains) as the value by which the model fails to correctly predict y. This is why its also sometimes called the *noise* of the model, or the *error*. 

Let's look at an example. Here we highlight the y value for the 15th observation in our data in blue. The residual is represented as a red line.

```{r}
i = 15
points(x[i], y[i], col='blue', pch=10)
segments(x[i], y[i], y1 = m$fitted.values[i], col='red')
```

For this observation, the regression formula is:

$$\color{blue}{y_i} = \color{green}{\beta_0 + \beta_1{x}_i} + \color{red}{\epsilon_i}$$
$$\color{blue}{5.90} = \color{green}{3.05 + 0.29*7.37 + \color{red}{0.68}$$

Understanding the relation between the prediction (the structural component) and the residual (the random component) also paves the way to undestanding $R2$ measure. This is the  

# Exercise

As an excersise, please 